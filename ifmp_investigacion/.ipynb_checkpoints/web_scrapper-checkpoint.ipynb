{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457ea62-4589-475f-90d2-54d9286a3a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Aug  1 10:43:34 2021\n",
    "\n",
    "@author: IgVinçon\n",
    "\"\"\"\n",
    "\n",
    "############################ IMPORT LIBRARIES ################################\n",
    "import json\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0abfc5-5a3a-47a3-8870-ddacec03735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the following lines are surrounded by the comment hashtag, these\n",
    "# contain code to test everything is working as intended. Uncomment them in\n",
    "# case of testing yourself or checking the line of reasoning.\n",
    "############################# ANII SCRAPPING #################################\n",
    "# ANII's website (https://anii.org.uy/proyectos/) uses JavaScript to show more\n",
    "# projects. By heading to the website > right click > 'inspect' > 'network' and\n",
    "# filtering by XHR there is a \"getMoreProjects.php\" file, its' response\n",
    "# contains the projects data to scrape.\n",
    "\n",
    "# Assign \"getMoreProjects.php\" URL and pass it to make_request. \n",
    "# The response is a json dictionary, parse it as such.\n",
    "anii_url = 'https://anii.org.uy/app/ajax/frontend/getMoreProjects.php'\n",
    "r, sleep = scrap.make_request(anii_url)\n",
    "content = r.json()\n",
    "##############################################################################\n",
    "# Check it is a dictionary and look at its keys:\n",
    "#print(type(content))\n",
    "#print(content.keys())\n",
    "# The 'html' key has the data to extract, 'numRows' key provides the current\n",
    "# rows, while 'total' key refers to the total rows in the whole table.\n",
    "##############################################################################\n",
    "\n",
    "# Create a payload (to pass into make_request) identical to getMoreProjects.php\n",
    "# request in order to emulate it. Passing the value from content['total'] to\n",
    "# the 'count' key gets the whole table.\n",
    "data = {\n",
    "    'offset': '0',\n",
    "    'count': content['total'],\n",
    "    'order': 'anioconvocatoria',\n",
    "    'criteria': 'DESC',\n",
    "    'qry': '',\n",
    "    'departamento': '',\n",
    "    'estado': '',\n",
    "    'fase': '',\n",
    "    'instrumento': ''\n",
    "}\n",
    "# The other keys seem to be filters that can be manipulated if desired.\n",
    "\n",
    "# Make (POST) request (with data as payload) and parse the response.\n",
    "r, sleep = scrap.make_request(anii_url, sleep, data)\n",
    "content = r.json()\n",
    "content = BeautifulSoup(content['html'], 'lxml')\n",
    "\n",
    "# Create a dictionary to store the data when scrapped.\n",
    "anii_dict = {\n",
    "    'instrumento': [],       \n",
    "    'beneficiario': [],\n",
    "    'departamento': [],\n",
    "    'subsidio': [],\n",
    "    'fecha de inicio': [],\n",
    "    'duracion': [],\n",
    "    'codigo': [],            \n",
    "    'proyecto': [],\n",
    "    'area': [],\n",
    "    'anio': [],\n",
    "    'fase_estado': [],       \n",
    "    'area de proy.': [],\n",
    "    'sector': [],\n",
    "#    'resumen': [],   Avoid it in this version, has many edge cases to account for.\n",
    "    'enlace': []    \n",
    "}\n",
    "\n",
    "# Since the data being extracted is in a table, each row is encapsulated\n",
    "# between <tr></tr>, which in turn has encapsulated each cell between <a></a>.\n",
    "rows = content.select('tr')\n",
    "##############################################################################\n",
    "# Testing the total number of rows is correct.\n",
    "#total_rows = int(data['count'])\n",
    "#print(f\"Nº of rows is correct: {len(rows) == total_rows}\")\n",
    "##############################################################################\n",
    "\n",
    "# Create a list of keys to pass to table_scrapper. Afterwards, clean the last\n",
    "# row of the table since it is a placeholder (confirming before removing).\n",
    "keys = [\n",
    "    'proyecto',\n",
    "    'beneficiario',\n",
    "    'area',\n",
    "    'fase_estado',\n",
    "    'anio'\n",
    "]\n",
    "anii_dict = scrap.table_scrapper(rows, 'a', anii_dict, keys, {'enlace': 0})\n",
    "keys.append('enlace')\n",
    "for k in keys:\n",
    "##############################################################################\n",
    "#    print(anii_dict[k][-1])\n",
    "    anii_dict[k] = anii_dict[k][:-1]\n",
    "#    print(anii_dict[k][-1])\n",
    "\n",
    "# Dictionary to keep track of URLs scrapped, if there was redirection and,\n",
    "# in case of errors, easly accessing said URL.\n",
    "#testing_dict = {\n",
    "#    'case_num': [],\n",
    "#    'correct_url': [],\n",
    "#    'url': []\n",
    "#}\n",
    "##############################################################################\n",
    "######                                                                 #######\n",
    "###### WARNING!!! Executing this loop will take between 24-48hs aprox. #######\n",
    "###### To execute in parts, isolate the code in a notebook cell, pass  #######\n",
    "###### ranges (e.g. [:anii_dict['enlace']//4] and so on) and save to   #######\n",
    "###### hardrive intermediate results with json module or some other.   #######\n",
    "###### For instance:                                                   #######\n",
    "#with open(\"anii.json\", \"w\") as fp:       # To save intermediate results\n",
    "#    json.dump(anii_dict, fp)\n",
    "#with open(\"anii.json\", \"r\") as fp:       # To load intermediate results\n",
    "#    anii_dict = json.load(fp)\n",
    "\n",
    "# Create list of keywords to pass to linear_search in the for loop.\n",
    "keywords = [k for k in anii_dict.keys() if k not in keys]\n",
    "for i in tqdm(range(len(anii_dict['enlace']))):\n",
    "    # Make request and parse it.\n",
    "    r, sleep = scrap.make_request(anii_dict['enlace'][i], sleep)\n",
    "    content = BeautifulSoup(r.content, 'lxml')\n",
    "##############################################################################\n",
    "    #testing_dict['case_num'].append(i)\n",
    "    #testing_dict['correct_url'].append(r.url == anii_dict['enlace'][i])\n",
    "    #testing_dict['url'].append(anii_dict['enlace'][i])\n",
    "##############################################################################    \n",
    "\n",
    "    # If the URL requested is the correct one, start scrapping.\n",
    "    if r.url == anii_dict['enlace'][i]:\n",
    "        # Select the data (two halves of a block, a comment and abstract).\n",
    "        left_half = (\n",
    "            content\n",
    "            .find_all('ul', class_ = 'content_details')[0]\n",
    "            .select('li')\n",
    "        )\n",
    "        right_half = (\n",
    "            content\n",
    "            .find_all('ul', class_ = 'content_details')[1]\n",
    "            .select('li')\n",
    "        )           \n",
    "        comment = (\n",
    "            content\n",
    "            .find_all('ul', class_ = 'content_details')[0]\n",
    "            .find(text = lambda text: isinstance(text, Comment))\n",
    "        )\n",
    "        comment = BeautifulSoup(comment, 'lxml')\n",
    "        comment = comment.select('li')\n",
    "        #abstract = content.find('div', class_ = 'content_info')\n",
    "        #abstract = scrap.extract_text(abstract)#.split(maxsplit = 2)[-1]\n",
    "        \n",
    "        # Scrape and store the data using linear_search while taking into\n",
    "        # account some edge cases. \n",
    "        for keyword in keywords: # Buscar una manera de simplificar y que no quede tres veces código similar (es posible??)\n",
    "            #print(f\"keyword is: {keyword}\")\n",
    "            if keyword in keywords[:3]:\n",
    "                k, v = scrap.linear_search(left_half, keyword, True)\n",
    "            elif keyword in keywords[3:6]:\n",
    "                k, v = (\n",
    "                    scrap.linear_search(right_half, keyword, True)\n",
    "                    if keyword != 'fecha de inicio'\n",
    "                    else scrap.linear_search(right_half, keyword, True, True)\n",
    "                )\n",
    "            else:\n",
    "                k, v = (\n",
    "                    scrap.linear_search(comment, keyword, True)\n",
    "                    if keyword != 'area_de_proy.'\n",
    "                    else scrap.linear_search(comment, keyword, True, True)\n",
    "                )\n",
    "            #print(f\"key: {k}, value: {v}\")\n",
    "            anii_dict[k].append(v)\n",
    "    \n",
    "    # If there was redirection, append 'n/a'.\n",
    "    else:\n",
    "        for keyword in keywords:\n",
    "              anii_dict[keyword].append('n/a')\n",
    "\n",
    "##############################################################################\n",
    "# Check percentage of webpages scrapped and, in case of errors, the last page\n",
    "# that was accessed.\n",
    "#true_count = sum(x == True for x in testing_dict['correct_url'])\n",
    "#percentage = true_count/len(testing_dict['correct_url'])\n",
    "#print(f\"Percentage of retrieved url: {percentage:.2%}\")\n",
    "#print(f\"Case number of last URL retrieved: {testing_dict['case_num'][-1]}\")\n",
    "#for_loop_result = testing_dict['case_num'][-1] == len(anii_dict['id'])\n",
    "#print(f\"Have all URL been accessed? {for_loop_result}\")\n",
    "#if not(for_loop_result):\n",
    "    #print(f\"Error found in: {testing_df['url'][-1]}\")\n",
    "##############################################################################\n",
    "              \n",
    "# Create DataFrame from the dictionary and save it as .csv \n",
    "#anii_df = pd.DataFrame(data = anii_dict)\n",
    "#anii_df.to_csv('anii.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f9b153-723e-45d5-a294-ebfa6fe91ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## SNI SCRAPPING #################################\n",
    "# Make request to SNI website and parse the content.\n",
    "sni_url = 'https://sni.org.uy/buscador'\n",
    "r, sleep = scrap.make_request(sni_url)\n",
    "content = BeautifulSoup(r.content, 'lxml')\n",
    "# Select the table rows ('tr'), except the first which is empty.\n",
    "rows = content.select('tr')[1:]\n",
    "\n",
    "# Create a SNI dictionary with the appropiate columns from the table.\n",
    "sni_dict = {\n",
    "    'nombre': [],\n",
    "    'nivel': [],\n",
    "    'categoria': [],\n",
    "    'area': [],\n",
    "    'subarea': []\n",
    "}\n",
    "##############################################################################\n",
    "# Find the total rows number according to the site\n",
    "#total_rows = content.find('div', class_ = 'encontados_buscador')\n",
    "#total_rows = int(total_rows.text.split(maxsplit = 2)[1])\n",
    "# And check total it is equal to the length of the rows variable\n",
    "#print(f\"Nº of rows is correct: {len(rows) == total_rows}\")\n",
    "##############################################################################\n",
    "# Create a list with the dictionary keys to pass to table_scrapper function.\n",
    "# In this case, each cell is between a 'td' tag and there is no URL to extract.\n",
    "keys = [k for k in sni_dict.keys()]\n",
    "sni_dict = scrap.table_scrapper(rows, 'td', sni_dict, keys)\n",
    "\n",
    "# Create DataFrame from the dictionary and save it as .csv \n",
    "sni_df = pd.DataFrame(data = sni_dict)\n",
    "sni_df.to_csv('sni.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b60fc82-ca33-45c3-b7fc-7aa1b5be1d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## EI SCRAPPING ##################################\n",
    "# All three URLs from EI have the same format, thus only one dictionary\n",
    "# (with the same columns) is needed.\n",
    "ei_dict = {\n",
    "    'grupo': [],\n",
    "    'convocatoria': [],\n",
    "    'periodo': [],\n",
    "    'url_grupo': [],\n",
    "    'url_convo': []\n",
    "}\n",
    "# Create a list with the dictionary keys to pass to table_scrapper function.\n",
    "href = {'url_grupo': 0,\n",
    "        'url_convo': 2}\n",
    "keywords = ['descripcion', 'objetivos', 'servicios involucrados', 'responsables']\n",
    "keys = [k for k in ei_dict.keys() if k not in href.keys()]\n",
    "for keyword in keywords:\n",
    "    ei_dict[keyword] = []\n",
    "\n",
    "# Since all URLs are identical, create a list with them to loop later.\n",
    "base_url = 'https://ei.udelar.edu.uy'\n",
    "ei_urls = [\n",
    "    (\n",
    "    r'https://ei.udelar.edu.uy/programa-financiamiento/'\n",
    "    r'programa-de-apoyo-centros-interdisciplinarios-de-la-udelar'\n",
    "    ), (\n",
    "    r'https://ei.udelar.edu.uy/programa-financiamiento/'\n",
    "    r'programa-nucleos-interdisciplinarios'\n",
    "    ), (\n",
    "    r'https://ei.udelar.edu.uy/programa-financiamiento/'\n",
    "    r'semillero-de-iniciativas-interdisciplinarias'\n",
    "    )\n",
    "]\n",
    "\n",
    "sleep = 0\n",
    "for i in tqdm(range(len(ei_urls))):\n",
    "    # Create a copy of the URL in order to update it\n",
    "    url_copy = ei_urls[i]\n",
    "    # NOTE: The progress bar for each URL always shows total web pages - 1.\n",
    "    # Thus, for the 3 pages for \"Centros\", the progress bar shows 2 (and so on).\n",
    "    \n",
    "    for _ in tqdm(scrap.generator()):\n",
    "        r, sleep = scrap.make_request(url_copy, sleep)\n",
    "        content = BeautifulSoup(r.content, 'lxml')\n",
    "        # Try to scrape and update URL to continue scrapping                  \n",
    "        try:\n",
    "            # Select the table rows ('tr'), except the first.\n",
    "            rows = content.select('tr')[1:]\n",
    "            # Find the total rows number according to the site\n",
    "        \n",
    "            # In this case, each cell is between a 'td' tag\n",
    "            ei_dict = scrap.table_scrapper(\n",
    "                rows, 'td', ei_dict, keys, href, base_url\n",
    "            )\n",
    "            # After extraction, get the next page,\n",
    "            next_page = content.main.section.div.select(\n",
    "                'li.pager__item.pager__item--next > a'\n",
    "            )[0]\n",
    "            # extract its URL and update the make_request URL \n",
    "            next_page = scrap.extract_href(next_page)\n",
    "            url_copy = ei_urls[i] + next_page                       \n",
    "    \n",
    "        # When there is nothing to scrape (a table or next page) finish the loop                   \n",
    "        except:\n",
    "            break\n",
    "            \n",
    "for i in tqdm(range(len(ei_dict['url_grupo']))):\n",
    "    r, sleep = scrap.make_request(ei_dict['url_grupo'][i], sleep)\n",
    "    content = BeautifulSoup(r.content, 'lxml')\n",
    "    div = (\n",
    "        content\n",
    "        .main\n",
    "        .section\n",
    "        .div\n",
    "        .find_all('div')[3]\n",
    "        .div\n",
    "        .article\n",
    "        .div.find_all('div')\n",
    "    )\n",
    "    for keyword in keywords:\n",
    "        k, v = scrap.linear_search(div, keyword)\n",
    "        ei_dict[k].append(v)    \n",
    "        \n",
    "# Create DataFrame from the dictionary and save it as .csv \n",
    "#ei_df = pd.DataFrame(data = ei_dict)\n",
    "#ei_df.to_csv('ei.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ffc94a-c5e2-4456-af6e-598175745a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# CSIC SCRAPPING #################################\n",
    "url = 'https://www.csic.edu.uy/proyectos-financiados'\n",
    "base_url = 'https://www.csic.edu.uy'\n",
    "\n",
    "csic_dict = {\n",
    "    'proyecto': [],\n",
    "    'programa': [], #Se podría extraer URL, pero lleva al programa con todas las convocatorias, no de un año especifico.\n",
    "    'ano': [],\n",
    "    'area proyecto': [],\n",
    "    'resumen': [],\n",
    "    'responsables': [],\n",
    "    'monto total': [],\n",
    "    'enlace': []\n",
    "}\n",
    "\n",
    "url_copy = url\n",
    "sleep = 0\n",
    "for _ in tqdm(scrap.generator()):\n",
    "    r, sleep = scrap.make_request(url_copy, sleep)\n",
    "    content = BeautifulSoup(r.content, 'lxml')\n",
    "    \n",
    "    try:\n",
    "        hrefs = content.find_all('div', class_ = 'fusion-button-wrapper')\n",
    "        for href in hrefs:\n",
    "            scrapped_url = base_url + scrap.extract_href(href.find('a'))\n",
    "            csic_dict['enlace'].append(scrapped_url)\n",
    "        next_page = content.select('li.next > a')[0]\n",
    "        next_page = scrap.extract_href(next_page)\n",
    "        url_copy = base_url + next_page\n",
    "    \n",
    "    except:\n",
    "        break\n",
    "##############################################################################    \n",
    "#total_entries = content.select('section > div > div.view-footer')[0]\n",
    "#total_entries = scrap.extract_text(total_entries)\n",
    "#total_entries = int(total_entries.split()[-1])\n",
    "#for_loop_result = total_entries == len(csic_dict['enlace'])\n",
    "#print(f\"Scrapped URLs and total_entries are the same? {for_loop_result}\")\n",
    "##############################################################################\n",
    "\n",
    "keywords = [\n",
    "    k for k in csic_dict.keys() \n",
    "    if k not in ['proyecto', 'enlace', 'resumen']\n",
    "]\n",
    "    \n",
    "for i in tqdm(range(len(csic_dict['enlace']))):\n",
    "    r, sleep = scrap.make_request(csic_dict['enlace'][i], sleep)\n",
    "    content = BeautifulSoup(r.content, 'lxml')\n",
    "    \n",
    "    title = scrap.extract_text(content.select('h1')[0])\n",
    "    div = content.section.article.select('div')\n",
    "    abstract = scrap.extract_text(\n",
    "        content.section.article.select('div.field-item.even')[3]\n",
    "    )\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        k, v = scrap.linear_search(div, keyword + ':')\n",
    "        k = k.split(':')[0]\n",
    "        csic_dict[k].append(v)\n",
    "        \n",
    "    csic_dict['proyecto'].append(title)\n",
    "    csic_dict['resumen'].append(abstract)\n",
    "    \n",
    "##############################################################################    \n",
    "#for k in csic_dict.keys():\n",
    "#    length = len(csic_dict[k])\n",
    "#    result = total_entries == length\n",
    "#    print(f\"length of {k} and total_entries are the same? {result}\")\n",
    "##############################################################################\n",
    "# Create DataFrame from the dictionary and save it as .csv \n",
    "#csic_df = pd.DataFrame(data = csic_dict)\n",
    "#csic_df.to_csv('csic.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
