{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457ea62-4589-475f-90d2-54d9286a3a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This is the example module.\n",
    "\n",
    "This module does stuff.\n",
    "Created on Sun Aug  1 10:43:34 2021\n",
    "\"\"\"\n",
    "\n",
    "__version__ = '0.5'\n",
    "__author__ = 'Juan Ignacio Rodríguez Vinçon'\n",
    "\n",
    "############################ IMPORT LIBRARIES ################################\n",
    "import json\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import scrap\n",
    "\n",
    "############################## SUB ROUTINES ##################################\n",
    "def anii_scrapping(dict, lower_bound, upper_bound, keywords, test_dic = None):\n",
    "    \"\"\"docstring\"\"\"\n",
    "    \n",
    "    sleep = 0\n",
    "    for i in tqdm(range(lower_bound, upper_bound)):\n",
    "        # Make request and parse it.\n",
    "        r, sleep = scrap.make_request(dict['enlace'][i], sleep)\n",
    "        content = BeautifulSoup(r.content, 'lxml')\n",
    "        #######################################################################\n",
    "        #testing_dict['case_num'].append(i)\n",
    "        #testing_dict['correct_url'].append(r.url == dict['enlace'][i])\n",
    "        #testing_dict['url'].append(dict['enlace'][i])\n",
    "        #######################################################################    \n",
    "\n",
    "        # If the URL requested is the correct one, start scrapping.\n",
    "        if r.url == dict['enlace'][i]:\n",
    "            # Select the data (two halves of a block, a comment and abstract).\n",
    "            left_half = (\n",
    "                content\n",
    "                .find_all('ul', class_ = 'content_details')[0]\n",
    "                .select('li')\n",
    "            )\n",
    "            right_half = (\n",
    "                content\n",
    "                .find_all('ul', class_ = 'content_details')[1]\n",
    "                .select('li')\n",
    "            )           \n",
    "            comment = (\n",
    "                content\n",
    "                .find_all('ul', class_ = 'content_details')[0]\n",
    "                .find(text = lambda text: isinstance(text, Comment))\n",
    "            )\n",
    "            comment = BeautifulSoup(comment, 'lxml')\n",
    "            comment = comment.select('li')\n",
    "            #abstract = content.find('div', class_ = 'content_info')\n",
    "            #abstract = scrap.extract_text(abstract)#.split(maxsplit = 2)[-1]\n",
    "        \n",
    "            # Scrape and store the data using linear_search while taking into\n",
    "            # account some edge cases. \n",
    "            for keyword in keywords: # Buscar una manera de simplificar y que no quede tres veces código similar (es posible??)\n",
    "                #print(f\"keyword is: {keyword}\")\n",
    "                if keyword in keywords[:3]:\n",
    "                    k, v = scrap.linear_search(left_half, keyword, True)\n",
    "                elif keyword in keywords[3:6]:\n",
    "                    k, v = (\n",
    "                        scrap.linear_search(right_half, keyword, True)\n",
    "                        if keyword != 'fecha de inicio'\n",
    "                        else scrap.linear_search(right_half, keyword, True, True)\n",
    "                    )\n",
    "                else:\n",
    "                    k, v = (\n",
    "                        scrap.linear_search(comment, keyword, True)\n",
    "                        if keyword != 'area_de_proy.'\n",
    "                        else scrap.linear_search(comment, keyword, True, True)\n",
    "                    )\n",
    "                #print(f\"key: {k}, value: {v}\")\n",
    "                dict[k].append(v)\n",
    "    \n",
    "        # If there was redirection, append 'n/a'.\n",
    "        else:\n",
    "            for keyword in keywords:\n",
    "                dict[keyword].append('n/a')\n",
    "\n",
    "        ########################################################################\n",
    "        # Check percentage of webpages scrapped and, in case of errors, the\n",
    "        # last page that was accessed.\n",
    "        #true_count = sum(x == True for x in testing_dict['correct_url'])\n",
    "        #percentage = true_count/len(testing_dict['correct_url'])\n",
    "        #print(f\"Percentage of retrieved url: {percentage:.2%}\")\n",
    "        #print(\n",
    "        #   f\"Case number of last URL retrieved: {testing_dict['case_num'][-1]}\"\n",
    "        #)\n",
    "        #for_loop_result = testing_dict['case_num'][-1] == len(anii_dict['id'])\n",
    "        #print(f\"Have all URL been accessed? {for_loop_result}\")\n",
    "        #if not(for_loop_result):\n",
    "            #print(f\"Error found in: {testing_df['url'][-1]}\")\n",
    "        ##############################################################################\n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0abfc5-5a3a-47a3-8870-ddacec03735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the following lines are surrounded by the comment hashtag, these\n",
    "# contain code to test everything is working as intended. Uncomment them in\n",
    "# case of testing yourself or checking the line of reasoning.\n",
    "############################# ANII SCRAPPING #################################\n",
    "# ANII's website (https://anii.org.uy/proyectos/) uses JavaScript to show more\n",
    "# projects. By heading to the website > right click > 'inspect' > 'network' and\n",
    "# filtering by XHR there is a \"getMoreProjects.php\" file, its' response\n",
    "# contains the projects data to scrape.\n",
    "\n",
    "# Assign \"getMoreProjects.php\" URL and pass it to make_request. \n",
    "# The response is a json dictionary, parse it as such.\n",
    "anii_url = 'https://anii.org.uy/app/ajax/frontend/getMoreProjects.php'\n",
    "r, sleep = scrap.make_request(anii_url)\n",
    "content = r.json()\n",
    "##############################################################################\n",
    "# Check it is a dictionary and look at its keys:\n",
    "#print(type(content))\n",
    "#print(content.keys())\n",
    "# The 'html' key has the data to extract, 'numRows' key provides the current\n",
    "# rows, while 'total' key refers to the total rows in the whole table.\n",
    "##############################################################################\n",
    "\n",
    "# Create a payload (to pass into make_request) identical to getMoreProjects.php\n",
    "# request in order to emulate it. Passing the value from content['total'] to\n",
    "# the 'count' key gets the whole table.\n",
    "data = {\n",
    "    'offset': '0',\n",
    "    'count': content['total'],\n",
    "    'order': 'anioconvocatoria',\n",
    "    'criteria': 'DESC',\n",
    "    'qry': '',\n",
    "    'departamento': '',\n",
    "    'estado': '',\n",
    "    'fase': '',\n",
    "    'instrumento': ''\n",
    "}\n",
    "# The other keys seem to be filters that can be manipulated if desired.\n",
    "\n",
    "# Make (POST) request (with data as payload) and parse the response.\n",
    "r, sleep = scrap.make_request(anii_url, sleep, data)\n",
    "content = r.json()\n",
    "content = BeautifulSoup(content['html'], 'lxml')\n",
    "\n",
    "# Create a dictionary to store the data when scrapped.\n",
    "anii_dict = {\n",
    "    'instrumento': [],       \n",
    "    'beneficiario': [],\n",
    "    'departamento': [],\n",
    "    'subsidio': [],\n",
    "    'fecha de inicio': [],\n",
    "    'duracion': [],\n",
    "    'codigo': [],            \n",
    "    'proyecto': [],\n",
    "    'area': [],\n",
    "    'anio': [],\n",
    "    'fase_estado': [],       \n",
    "    'area de proy.': [],\n",
    "    'sector': [],\n",
    "#    'resumen': [],   Avoid it in this version, has many edge cases to account for.\n",
    "    'enlace': []    \n",
    "}\n",
    "\n",
    "# Since the data being extracted is in a table, each row is encapsulated\n",
    "# between <tr></tr>, which in turn has encapsulated each cell between <a></a>.\n",
    "rows = content.select('tr')\n",
    "##############################################################################\n",
    "# Testing the total number of rows is correct.\n",
    "#total_rows = int(data['count'])\n",
    "#print(f\"Nº of rows is correct: {len(rows) == total_rows}\")\n",
    "##############################################################################\n",
    "\n",
    "# Create a list of keys to pass to table_scrapper. Afterwards, clean the last\n",
    "# row of the table since it is a placeholder (confirming before removing).\n",
    "keys = [\n",
    "    'proyecto',\n",
    "    'beneficiario',\n",
    "    'area',\n",
    "    'fase_estado',\n",
    "    'anio'\n",
    "]\n",
    "anii_dict = scrap.table_scrapper(rows, 'a', anii_dict, keys, {'enlace': 0})\n",
    "keys.append('enlace')\n",
    "for k in keys:\n",
    "##############################################################################\n",
    "#    print(anii_dict[k][-1])\n",
    "    anii_dict[k] = anii_dict[k][:-1]\n",
    "#    print(anii_dict[k][-1])\n",
    "\n",
    "# Dictionary to keep track of URLs scrapped, if there was redirection and,\n",
    "# in case of errors, easly accessing said URL.\n",
    "#testing_dict = {\n",
    "#    'case_num': [],\n",
    "#    'correct_url': [],\n",
    "#    'url': []\n",
    "#}\n",
    "##############################################################################\n",
    "###### WARNING!!! This loop is executed in parts due to how many data  #######\n",
    "###### there is to scrape. It will save intermediate results in .json  #######\n",
    "###### format. It's recommended to execute at the start of a day since #######\n",
    "###### it will take around 9hs for each part to finish (at most, 12hs).#######\n",
    "##############################################################################\n",
    "\n",
    "# Create list of keywords to pass to linear_search in the for loop.\n",
    "keywords = [k for k in anii_dict.keys() if k not in keys]\n",
    "# Divide the total number of links by four and create four quarters\n",
    "# in order to pass each for the (in parts) extraction process.\n",
    "# Save them as a .json so as to recover them in the next execution.\n",
    "q1, q4 = len(anii_dict['enlace']) // 4, len(anii_dict['enlace'])\n",
    "q2, q3 = q1 * 2, q1 * 3\n",
    "anii_scrapping_parameters = {\n",
    "    'quarter_1': q1,\n",
    "    'quarter_2': q2,\n",
    "    'quarter_3': q3,\n",
    "    'quarter_4': q4,\n",
    "    'keywords': keywords\n",
    "}\n",
    "with open('anii_param.json', 'w') as fp:\n",
    "    json.dump(anii_scrapping_parameters, fp)\n",
    "\n",
    "# Use anii_scrapping function to start the scrapping process\n",
    "anii_dict = anii_scrapping(\n",
    "    anii_dict, \n",
    "    0, \n",
    "    anii_scrapping_parameters['quarter_1'], \n",
    "    anii_scrapping_parameters['keywords']\n",
    "    )\n",
    "# Save intermediate results in .json file\n",
    "with open(r'jsons\\anii.json', 'w') as fp:       \n",
    "    json.dump(anii_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c033a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## ANII SCRAPPING Pt. 2 ###############################\n",
    "# Load intermediate results\n",
    "with open(\"anii.json\", \"r\") as fp:       \n",
    "    anii_dict = json.load(fp)\n",
    "with open(\"anii_param.json\", \"r\") as fp:       \n",
    "    anii_scrapping_parameters = json.load(fp)\n",
    "\n",
    "# Use anii_scrapping function to continue the scrapping process\n",
    "anii_dict = anii_scrapping(\n",
    "    anii_dict, \n",
    "    anii_scrapping_parameters['quarter_1'], \n",
    "    anii_scrapping_parameters['quarter_2'], \n",
    "    anii_scrapping_parameters['keywords']\n",
    "    )\n",
    "# Save intermediate results in .json file\n",
    "with open(r'jsons\\anii.json', 'w') as fp:       \n",
    "    json.dump(anii_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c033a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## ANII SCRAPPING Pt. 3 ###############################\n",
    "# Load intermediate results\n",
    "with open(\"anii.json\", \"r\") as fp:       \n",
    "    anii_dict = json.load(fp)\n",
    "with open(\"anii_param.json\", \"r\") as fp:       \n",
    "    anii_scrapping_parameters = json.load(fp)\n",
    "\n",
    "# Use anii_scrapping function to continue the scrapping process\n",
    "anii_dict = anii_scrapping(\n",
    "    anii_dict, \n",
    "    anii_scrapping_parameters['quarter_2'], \n",
    "    anii_scrapping_parameters['quarter_3'], \n",
    "    anii_scrapping_parameters['keywords']\n",
    "    )\n",
    "# Save intermediate results in .json file\n",
    "with open(r'jsons\\anii.json', 'w') as fp:       \n",
    "    json.dump(anii_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c033a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## ANII SCRAPPING Pt. 4 ###############################\n",
    "# Load intermediate results\n",
    "with open(\"anii.json\", \"r\") as fp:       \n",
    "    anii_dict = json.load(fp)\n",
    "with open(\"anii_param.json\", \"r\") as fp:       \n",
    "    anii_scrapping_parameters = json.load(fp)\n",
    "\n",
    "# Use anii_scrapping function to continue the scrapping process\n",
    "anii_dict = anii_scrapping(\n",
    "    anii_dict, \n",
    "    anii_scrapping_parameters['quarter_3'], \n",
    "    anii_scrapping_parameters['quarter_4'], \n",
    "    anii_scrapping_parameters['keywords']\n",
    "    )\n",
    "# Save intermediate results in .json file\n",
    "with open(r'jsons\\anii.json', 'w') as fp:       \n",
    "    json.dump(anii_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f9b153-723e-45d5-a294-ebfa6fe91ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## SNI SCRAPPING #################################\n",
    "# Make request to SNI website and parse the content.\n",
    "sni_url = 'https://sni.org.uy/buscador'\n",
    "r, sleep = scrap.make_request(sni_url)\n",
    "content = BeautifulSoup(r.content, 'lxml')\n",
    "# Select the table rows ('tr'), except the first which is empty.\n",
    "rows = content.select('tr')[1:]\n",
    "\n",
    "# Create a SNI dictionary with the appropiate columns from the table.\n",
    "sni_dict = {\n",
    "    'nombre': [],\n",
    "    'nivel': [],\n",
    "    'categoria': [],\n",
    "    'area': [],\n",
    "    'subarea': []\n",
    "}\n",
    "##############################################################################\n",
    "# Find the total rows number according to the site\n",
    "#total_rows = content.find('div', class_ = 'encontados_buscador')\n",
    "#total_rows = int(total_rows.text.split(maxsplit = 2)[1])\n",
    "# And check total it is equal to the length of the rows variable\n",
    "#print(f\"Nº of rows is correct: {len(rows) == total_rows}\")\n",
    "##############################################################################\n",
    "# Create a list with the dictionary keys to pass to table_scrapper function.\n",
    "# In this case, each cell is between a 'td' tag and there is no URL to extract.\n",
    "keys = [k for k in sni_dict.keys()]\n",
    "sni_dict = scrap.table_scrapper(rows, 'td', sni_dict, keys)\n",
    "\n",
    "# Save results as .json \n",
    "with open(r'jsons\\sni.json', 'w') as fp:       \n",
    "    json.dump(sni_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b60fc82-ca33-45c3-b7fc-7aa1b5be1d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## EI SCRAPPING ##################################\n",
    "# All three URLs from EI have the same format, thus only one dictionary\n",
    "# (with the same columns) is needed.\n",
    "ei_dict = {\n",
    "    'grupo': [],\n",
    "    'convocatoria': [],\n",
    "    'periodo': [],\n",
    "    'url_grupo': [],\n",
    "    'url_convo': []\n",
    "}\n",
    "# Create a list with the dictionary keys to pass to table_scrapper function.\n",
    "href = {'url_grupo': 0,\n",
    "        'url_convo': 2}\n",
    "keywords = ['descripcion', 'objetivos', 'servicios involucrados', 'responsables']\n",
    "keys = [k for k in ei_dict.keys() if k not in href.keys()]\n",
    "for keyword in keywords:\n",
    "    ei_dict[keyword] = []\n",
    "\n",
    "# Since all URLs are identical, create a list with them to loop later.\n",
    "base_url = 'https://ei.udelar.edu.uy'\n",
    "ei_urls = [\n",
    "    (\n",
    "    r'https://ei.udelar.edu.uy/programa-financiamiento/'\n",
    "    r'programa-de-apoyo-centros-interdisciplinarios-de-la-udelar'\n",
    "    ), (\n",
    "    r'https://ei.udelar.edu.uy/programa-financiamiento/'\n",
    "    r'programa-nucleos-interdisciplinarios'\n",
    "    ), (\n",
    "    r'https://ei.udelar.edu.uy/programa-financiamiento/'\n",
    "    r'semillero-de-iniciativas-interdisciplinarias'\n",
    "    )\n",
    "]\n",
    "\n",
    "sleep = 0\n",
    "for i in tqdm(range(len(ei_urls))):\n",
    "    # Create a copy of the URL in order to update it\n",
    "    url_copy = ei_urls[i]\n",
    "    # NOTE: The progress bar for each URL always shows total web pages - 1.\n",
    "    # Thus, for the 3 pages for \"Centros\", the progress bar shows 2 (and so on).\n",
    "    \n",
    "    for _ in tqdm(scrap.generator()):\n",
    "        r, sleep = scrap.make_request(url_copy, sleep)\n",
    "        content = BeautifulSoup(r.content, 'lxml')\n",
    "        # Try to scrape and update URL to continue scrapping                  \n",
    "        try:\n",
    "            # Select the table rows ('tr'), except the first.\n",
    "            rows = content.select('tr')[1:]\n",
    "            # Find the total rows number according to the site\n",
    "        \n",
    "            # In this case, each cell is between a 'td' tag\n",
    "            ei_dict = scrap.table_scrapper(\n",
    "                rows, 'td', ei_dict, keys, href, base_url\n",
    "            )\n",
    "            # After extraction, get the next page,\n",
    "            next_page = content.main.section.div.select(\n",
    "                'li.pager__item.pager__item--next > a'\n",
    "            )[0]\n",
    "            # extract its URL and update the make_request URL \n",
    "            next_page = scrap.extract_href(next_page)\n",
    "            url_copy = ei_urls[i] + next_page                       \n",
    "    \n",
    "        # When there is nothing to scrape (a table or next page) finish the loop                   \n",
    "        except:\n",
    "            break\n",
    "            \n",
    "for i in tqdm(range(len(ei_dict['url_grupo']))):\n",
    "    r, sleep = scrap.make_request(ei_dict['url_grupo'][i], sleep)\n",
    "    content = BeautifulSoup(r.content, 'lxml')\n",
    "    div = (\n",
    "        content\n",
    "        .main\n",
    "        .section\n",
    "        .div\n",
    "        .find_all('div')[3]\n",
    "        .div\n",
    "        .article\n",
    "        .div.find_all('div')\n",
    "    )\n",
    "    for keyword in keywords:\n",
    "        k, v = scrap.linear_search(div, keyword)\n",
    "        ei_dict[k].append(v)    \n",
    "        \n",
    "# Save results as .json \n",
    "with open(r'jsons\\ei.json', 'w') as fp:       \n",
    "    json.dump(ei_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ffc94a-c5e2-4456-af6e-598175745a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# CSIC SCRAPPING #################################\n",
    "url = 'https://www.csic.edu.uy/proyectos-financiados'\n",
    "base_url = 'https://www.csic.edu.uy'\n",
    "\n",
    "csic_dict = {\n",
    "    'proyecto': [],\n",
    "    'programa': [], #Se podría extraer URL, pero lleva al programa con todas las convocatorias, no de un año especifico.\n",
    "    'ano': [],\n",
    "    'area proyecto': [],\n",
    "    'resumen': [],\n",
    "    'responsables': [],\n",
    "    'monto total': [],\n",
    "    'enlace': []\n",
    "}\n",
    "\n",
    "url_copy = url\n",
    "sleep = 0\n",
    "for _ in tqdm(scrap.generator()):\n",
    "    r, sleep = scrap.make_request(url_copy, sleep)\n",
    "    content = BeautifulSoup(r.content, 'lxml')\n",
    "    \n",
    "    try:\n",
    "        hrefs = content.find_all('div', class_ = 'fusion-button-wrapper')\n",
    "        for href in hrefs:\n",
    "            scrapped_url = base_url + scrap.extract_href(href.find('a'))\n",
    "            csic_dict['enlace'].append(scrapped_url)\n",
    "        next_page = content.select('li.next > a')[0]\n",
    "        next_page = scrap.extract_href(next_page)\n",
    "        url_copy = base_url + next_page\n",
    "    \n",
    "    except:\n",
    "        break\n",
    "##############################################################################    \n",
    "#total_entries = content.select('section > div > div.view-footer')[0]\n",
    "#total_entries = scrap.extract_text(total_entries)\n",
    "#total_entries = int(total_entries.split()[-1])\n",
    "#for_loop_result = total_entries == len(csic_dict['enlace'])\n",
    "#print(f\"Scrapped URLs and total_entries are the same? {for_loop_result}\")\n",
    "##############################################################################\n",
    "\n",
    "keywords = [\n",
    "    k for k in csic_dict.keys() \n",
    "    if k not in ['proyecto', 'enlace', 'resumen']\n",
    "]\n",
    "    \n",
    "for i in tqdm(range(len(csic_dict['enlace']))):\n",
    "    r, sleep = scrap.make_request(csic_dict['enlace'][i], sleep)\n",
    "    content = BeautifulSoup(r.content, 'lxml')\n",
    "    \n",
    "    title = scrap.extract_text(content.select('h1')[0])\n",
    "    div = content.section.article.select('div')\n",
    "    abstract = scrap.extract_text(\n",
    "        content.section.article.select('div.field-item.even')[3]\n",
    "    )\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        k, v = scrap.linear_search(div, keyword + ':')\n",
    "        k = k.split(':')[0]\n",
    "        csic_dict[k].append(v)\n",
    "        \n",
    "    csic_dict['proyecto'].append(title)\n",
    "    csic_dict['resumen'].append(abstract)\n",
    "    \n",
    "##############################################################################    \n",
    "#for k in csic_dict.keys():\n",
    "#    length = len(csic_dict[k])\n",
    "#    result = total_entries == length\n",
    "#    print(f\"length of {k} and total_entries are the same? {result}\")\n",
    "##############################################################################\n",
    "# Save results as .json \n",
    "with open(r'jsons\\csic.json', 'w') as fp:       \n",
    "    json.dump(csic_dict, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
